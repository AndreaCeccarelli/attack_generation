{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10Wf_UF0PRHx"
   },
   "source": [
    "# CICIDS adaptation\n",
    "\n",
    "conda activate notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6454,
     "status": "ok",
     "timestamp": 1616573880125,
     "user": {
      "displayName": "Angelica Liguori",
      "photoUrl": "",
      "userId": "05017064839845812601"
     },
     "user_tz": -60
    },
    "id": "yoBBT_CpPRHy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/notebook/anaconda3/envs/notebook/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] -> Using Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score,auc, roc_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support, auc, precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "device = torch.device('cuda' if (torch.cuda.is_available()) else 'cpu')\n",
    "print('[INFO] -> Using Device: ', device)\n",
    "\n",
    "from load_data import get_CICIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanAUC_PRAUC(auc_list, pr_list, name, SPACE_AUC, SPACE_AUPRC):\n",
    "    print('AUC:')\n",
    "    print(auc_list)\n",
    "    print('**************')\n",
    "    print('PR AUC:')\n",
    "    print(pr_list)\n",
    "    print('**************')\n",
    "    \n",
    "    \n",
    "    AUC_Frame = pd.DataFrame(auc_list, columns = [name])\n",
    "    PR_AUC_Frame = pd.DataFrame(pr_list, columns = [name])\n",
    "    \n",
    "    AU_NAME = f'{name}.csv'\n",
    "    \n",
    "    AUC_Frame.to_csv(os.path.join(SPACE_AUC, AU_NAME), index=False)\n",
    "    PR_AUC_Frame.to_csv(os.path.join(SPACE_AUPRC, AU_NAME), index=False)\n",
    "    \n",
    "    AUC_Frame = list(AUC_Frame[name])\n",
    "\n",
    "    N = len(AUC_Frame)\n",
    "    mean_auc = np.mean(AUC_Frame)\n",
    "    std_auc = np.std(AUC_Frame)\n",
    "    std_error = std_auc / (np.sqrt(N))\n",
    "\n",
    "    ci = 1.96 * std_error\n",
    "    lower_bound = mean_auc - ci\n",
    "    upper_bound = mean_auc + ci\n",
    "    \n",
    "    print('AUC')\n",
    "    print(f'{mean_auc:.2f} +/- {ci:.2f}')\n",
    "    print(f'95% confidence level, average auc would be between {lower_bound:.2f} and {upper_bound:.2f}')\n",
    "    print('**************')\n",
    "    \n",
    "    PR_AUC_Frame = list(PR_AUC_Frame[name])\n",
    "\n",
    "    N = len(PR_AUC_Frame)\n",
    "    mean_auc = np.mean(PR_AUC_Frame)\n",
    "    std_auc = np.std(PR_AUC_Frame)\n",
    "    std_error = std_auc / (np.sqrt(N))\n",
    "\n",
    "    ci = 1.96 * std_error\n",
    "    lower_bound = mean_auc - ci\n",
    "    upper_bound = mean_auc + ci\n",
    "    \n",
    "    print('PR AUC')\n",
    "    print(f'{mean_auc:.2f} +/- {ci:.2f}')\n",
    "    print(f'95% confidence level, average auc would be between {lower_bound:.2f} and {upper_bound:.2f}')\n",
    "    \n",
    "    \n",
    "def plotLoss(d_losses, g_losses,  bce_losses, rec_losses, kldes, real_scores, fake_scores, i, DATASET, show = False):\n",
    "    num_epochs = len(d_losses)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.xlim(0, num_epochs + 1)\n",
    "    plt.plot(range(1, num_epochs + 1), d_losses[:num_epochs], label='d loss')\n",
    "    plt.plot(range(1, num_epochs + 1), g_losses[:num_epochs], label='g loss')    \n",
    "\n",
    "#    plt.plot(range(1, num_epochs + 1), d_losses_val[:num_epochs], '--', label='d loss val')\n",
    "\n",
    "    plt.legend(loc = 'lower right', bbox_to_anchor=(.8, 0.3, 0.5, 0.5))\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(f'./images/{i}_losses_{DATASET}.pdf')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.xlim(0, num_epochs + 1)\n",
    "    plt.plot(range(1, num_epochs + 1), g_losses[:num_epochs], label='g loss')    \n",
    "    plt.plot(range(1, num_epochs + 1), bce_losses[:num_epochs], label=r'$\\log(D(y))$')    \n",
    "    plt.plot(range(1, num_epochs + 1), rec_losses[:num_epochs], label=r'MSE')    \n",
    "    plt.plot(range(1, num_epochs + 1), kldes[:num_epochs], label='KLD') \n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.legend(loc = 'lower right', bbox_to_anchor=(.8, 0.3, 0.5, 0.5))\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(f'./images/{i}_gen_loss_{DATASET}.pdf')  \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.xlim(0, num_epochs + 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.plot(range(1, num_epochs + 1), fake_scores[:num_epochs], label='fake score')\n",
    "    plt.plot(range(1, num_epochs + 1), real_scores[:num_epochs], label='real score')    \n",
    "\n",
    "    plt.legend(loc = 'lower right', bbox_to_anchor=(.8, 0.3, 0.5, 0.5))\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(f'./images/{i}_score_{DATASET}.pdf') \n",
    "        \n",
    "\n",
    "def generate_labels(size,pflip,lb,ub,step,decay=.9995,up=True): #decay=.99995\n",
    "    \n",
    "    if up:\n",
    "        lb = ub - (ub-lb)*((decay)**step)\n",
    "    else:\n",
    "        ub = lb + (ub-lb)*((decay)**step)\n",
    "    pflip = pflip*((decay)**step)\n",
    "    \n",
    "    y = np.random.uniform(lb, ub,size)   \n",
    "\n",
    "    sf = int(pflip*size)    \n",
    "    if sf > 0:\n",
    "        y[:sf] = 1- y[:sf]\n",
    "        np.random.shuffle(y)\n",
    "    \n",
    "    return torch.FloatTensor(y)\n",
    "\n",
    "\n",
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    U = torch.rand(shape).to(device)\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    y = torch.log_softmax(logits, dim=-1) + sample_gumbel(logits.size())\n",
    "    return torch.softmax(y / temperature, dim=-1).to(device)\n",
    "\n",
    "\n",
    "def gumbel_softmax(logits, temperature=1e-5):\n",
    "    \"\"\"\n",
    "    input: [*, n_class]\n",
    "    return: [*, n_class] an one-hot vector\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    shape = y.size()\n",
    "    _, ind = y.max(dim=-1)\n",
    "    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "    y_hard = y_hard.view(*shape)\n",
    "    return (y_hard - y).detach() + y\n",
    "\n",
    "\n",
    "def gumbel_sigmoid_sample(logits, temperature):\n",
    "    # See https://davidstutz.de/categorical-variational-auto-encoders-and-the-gumbel-trick/\n",
    "    u = torch.rand_like(logits)\n",
    "    # we exploit the fact log(sigma(x)) - log(1-sigma(x)) = x\n",
    "    y = logits + torch.log(u) - torch.log(1 - u)\n",
    "    \n",
    "    return torch.sigmoid(y / temperature)\n",
    "\n",
    "\n",
    "def gumbel_sigmoid(logits, temperature=1e-5):\n",
    "    \"\"\"\n",
    "    input: [*]\n",
    "    return: [*] a binary response\n",
    "    \"\"\"\n",
    "    y = gumbel_sigmoid_sample(logits, temperature)\n",
    "    y_hard = (y > .5).float()\n",
    "    return (y_hard - y).detach() + y\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nf_in = 121, nf_out = 32, z_dim = 16):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.nf_in = nf_in\n",
    "        self.nf_out = nf_out\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.nf_in, self.nf_out * 2), \n",
    "            nn.BatchNorm1d(self.nf_out * 2, track_running_stats = False),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Linear(self.nf_out * 2, self.nf_out),\n",
    "            nn.BatchNorm1d(self.nf_out, track_running_stats = False),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.nf_out, self.nf_out * 2),\n",
    "            nn.BatchNorm1d(self.nf_out * 2, track_running_stats = False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(self.nf_out * 2, self.nf_in)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(self.nf_out, self.nf_out)\n",
    "        self.fc21 = nn.Linear(self.nf_out, self.z_dim)\n",
    "        self.fc22 = nn.Linear(self.nf_out, self.z_dim)\n",
    "\n",
    "        self.fc3 = nn.Linear(self.z_dim, self.nf_out)\n",
    "        self.fc4 = nn.Linear(self.nf_out, self.nf_out)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "        self.init_weights()  \n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + std * eps\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc21(h), self.fc22(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        conv = self.encoder(x)\n",
    "        h = self.fc1(conv)\n",
    "        \n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.relu(self.fc3(z))\n",
    "        deconv_input = self.fc4(h)\n",
    "        \n",
    "        return self.decoder(deconv_input)\n",
    "    \n",
    "    def gumbel(self, logits, t):\n",
    "        return gumbel_softmax(logits, t)\n",
    "\n",
    "    def forward(self, x, text_l, selected_columnsTrain, index, t=1):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        logits = self.decode(z)\n",
    "        \n",
    "        sampled_data = logits.clone()\n",
    "        sampled_data[:, index] = self.sigmoid(logits[:, index])\n",
    "        for name in text_l:\n",
    "            sampled_data[:, selected_columnsTrain[name]] = self.gumbel(logits[:, selected_columnsTrain[name]], t)\n",
    "        \n",
    "        return logits, mu, logvar, sampled_data\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, nc = 121, nf_out = 16, nout = 128):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.nc = nc\n",
    "        self.nf_out = nf_out\n",
    "        self.nout = nout\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            # features extractor\n",
    "            nn.Linear(self.nc, self.nout),\n",
    "            nn.BatchNorm1d(self.nout, track_running_stats = False),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Linear(self.nout, self.nout * 2),\n",
    "            nn.BatchNorm1d(self.nout * 2, track_running_stats = False),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Linear(self.nout * 2, self.nout * 4),\n",
    "            nn.BatchNorm1d(self.nout * 4, track_running_stats = False),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # classifier\n",
    "            nn.Linear(self.nout * 4, self.nout),\n",
    "            nn.BatchNorm1d(self.nout, track_running_stats = False),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(self.nout, self.nf_out * 4),\n",
    "            nn.BatchNorm1d(self.nf_out * 4, track_running_stats = False),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.nf_out * 4, self.nf_out * 2),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.nf_out * 2, self.nf_out),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.nf_out, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.init_weights()  \n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        return x.flatten()\n",
    "    \n",
    "\n",
    "class DiscriminatorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorLoss, self).__init__()\n",
    "        \n",
    "        self.criterion = nn.BCELoss() \n",
    "        \n",
    "    def forward(self, true_preds, fake_preds, step):\n",
    "        \n",
    "        bs = true_preds.size(0)\n",
    "        y_real = generate_labels(bs,0.3,0.7,1., step, up=True).to(device)\n",
    "    \n",
    "        D_real_loss = self.criterion(true_preds, y_real)\n",
    "    \n",
    "        y_fake = generate_labels(bs,0.3,0.,0.3, step, up=False).to(device)\n",
    "\n",
    "        D_fake_loss = self.criterion(fake_preds, y_fake)\n",
    "\n",
    "        return D_real_loss + D_fake_loss\n",
    "    \n",
    "    \n",
    "class GeneratorLoss(nn.Module):\n",
    "    def __init__(self, text_l, selected_columnsTrain, index):\n",
    "        super(GeneratorLoss, self).__init__()\n",
    "        \n",
    "        self.text_l = text_l\n",
    "        self.selected_columnsTrain = selected_columnsTrain\n",
    "        self.index = index\n",
    "        \n",
    "        self.criterion = nn.BCELoss() \n",
    "        self.mse = nn.MSELoss(reduction = 'mean')\n",
    "        self.cel = nn.CrossEntropyLoss()\n",
    "        \n",
    "\n",
    "    def KLD(self,z_mean, z_logvar):\n",
    "        return torch.mean(0.5 * (-0.5 * z_logvar + torch.exp(0.5 * z_logvar) + z_mean ** 2))\n",
    "    \n",
    "    \n",
    "    def reconstruction(self, true_data, sampled_data):\n",
    "        g1 = self.mse(true_data[:, self.index], sampled_data[:, self.index])\n",
    "        g2 = 0\n",
    "        for name in self.text_l:\n",
    "            y = true_data[:, self.selected_columnsTrain[name]]\n",
    "            y_p = sampled_data[:, self.selected_columnsTrain[name]]\n",
    "            g2 += self.mse(y_p, y)\n",
    "        g2 /= len(self.text_l)\n",
    "        return g1 + 0.5 * g2\n",
    "\n",
    "    def forward(self, true_data, fake_preds, sampled_data, z_mean, z_logvar, beta = 1, gamma = 1e-2):\n",
    "        \n",
    "        bs = fake_preds.size(0)\n",
    "                \n",
    "        y_fake = torch.ones(bs).to(device)\n",
    "        log_p_y = self.criterion(fake_preds, y_fake)\n",
    "        \n",
    "        rec = self.reconstruction(true_data, sampled_data)\n",
    "        \n",
    "        kld = self.KLD(z_mean, z_logvar)\n",
    "        \n",
    "        return gamma*log_p_y + rec + beta*kld, log_p_y, rec, kld\n",
    "    \n",
    "    \n",
    "    \n",
    "class AADNet(nn.Module):\n",
    "    def __init__(self, device, selected_columns, discreteCol, index, nc):\n",
    "        super(AADNet, self).__init__()\n",
    "        self.device = device\n",
    "        self.selected_columns = selected_columns\n",
    "        self.discreteCol = discreteCol\n",
    "        self.index = index\n",
    "        self.nc = nc\n",
    "\n",
    "        self.D = Discriminator(nc = self.nc).to(self.device)\n",
    "        self.G = Generator(nf_in = self.nc).to(self.device)\n",
    "\n",
    "        self.d_optimizer = torch.optim.Adam(self.D.parameters(), lr=1e-5)\n",
    "        self.g_optimizer = torch.optim.Adam(self.G.parameters(), lr=1e-4)\n",
    "        \n",
    "        self.d_loss = DiscriminatorLoss()\n",
    "        self.g_loss = GeneratorLoss(self.discreteCol, self.selected_columns, self.index)\n",
    "        \n",
    "        self.temperature = 1\n",
    "        self.anneal = 0.9995\n",
    "        \n",
    "        self.criterion = nn.BCELoss()\n",
    "        \n",
    "        \n",
    "    def D_step(self,true_data, step):\n",
    "        self.D.zero_grad()\n",
    "\n",
    "        logits, _, _, sampled_data = self.G(true_data, self.discreteCol, self.selected_columns, self.index, self.temperature)\n",
    "        true_pred = self.D(true_data)\n",
    "        fake_pred = self.D(sampled_data.detach())\n",
    "\n",
    "        d_loss_batch = self.d_loss(true_pred, fake_pred, step)\n",
    "        d_loss_batch.backward()\n",
    "        self.d_optimizer.step()\n",
    "\n",
    "        return d_loss_batch, true_pred, fake_pred\n",
    "\n",
    "    \n",
    "    def G_step(self,true_data, step):\n",
    "        self.G.zero_grad()\n",
    "        \n",
    "        logits, z_mean, z_logvar, sampled_data = self.G(true_data, self.discreteCol, self.selected_columns, \n",
    "                                                        self.index, self.temperature)    \n",
    "        fake_pred = self.D(sampled_data)\n",
    "\n",
    "        gen_loss_batch, bce_loss, rec_loss, kl = self.g_loss(true_data, \n",
    "                                                             fake_pred, \n",
    "                                                             sampled_data, \n",
    "                                                             z_mean, \n",
    "                                                             z_logvar, \n",
    "                                                             self.temperature)\n",
    "        gen_loss_batch.backward()\n",
    "\n",
    "        self.g_optimizer.step()\n",
    "\n",
    "        return gen_loss_batch, bce_loss, rec_loss, kl\n",
    "\n",
    "    \n",
    "    def anneal_temp(self, lowerbound=1e-5):\n",
    "        if self.temperature > lowerbound:\n",
    "            self.temperature = self.temperature*self.anneal\n",
    "    \n",
    "    def plot_pr_curve(self, precision, recall):\n",
    "        plt.figure()\n",
    "        plt.plot(recall, precision, marker='.')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.show()\n",
    "    \n",
    "    def pr_auc(self, y_test, y_pred):\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "        auc_score = auc(recall, precision)\n",
    "        print(f'PR AUC: {auc_score:.2f}')\n",
    "        self.plot_pr_curve(precision, recall)\n",
    "        return auc_score\n",
    "    \n",
    "            \n",
    "    def evaluation(self, test_loader):\n",
    "        self.D.eval()\n",
    "        \n",
    "        d_l = []\n",
    "        ind = 0\n",
    "        \n",
    "        for batch, label in test_loader:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                y_pred = self.D(batch)\n",
    "                \n",
    "                d_loss_batch = self.criterion(y_pred, label)\n",
    "                \n",
    "            d_l.append(d_loss_batch.item())\n",
    "        \n",
    "        return np.mean(d_l)\n",
    "    \n",
    "    def predict(self, test_loader):\n",
    "        self.D.eval()\n",
    "        i = 0\n",
    "        \n",
    "        for batch, label in test_loader:\n",
    "            batch = batch.to(self.device)\n",
    "            label = label.to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                y_pred = self.D(batch)\n",
    "                \n",
    "            if i == 0:\n",
    "                prediction = y_pred.cpu().round()\n",
    "                y_true = label.cpu()\n",
    "                yP = y_pred.cpu()\n",
    "            else:\n",
    "                prediction = torch.cat((prediction, y_pred.cpu().round()))\n",
    "                y_true = torch.cat((y_true, label.cpu()))\n",
    "                yP = torch.cat((yP, y_pred.cpu()))\n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "        return prediction, y_true, yP\n",
    "        \n",
    "        \n",
    "    def train(self, data_loader, test_loader, batch_size = 32, num_epochs = 10, step = 10, \n",
    "              lowerbnd=5e-15, num_q_steps = 1, num_g_steps = 1):\n",
    "        \n",
    "        d_losses = np.zeros(num_epochs)\n",
    "        g_losses = np.zeros(num_epochs)\n",
    "        real_scores = np.zeros(num_epochs)\n",
    "        fake_scores = np.zeros(num_epochs)\n",
    "        rec_losses = np.zeros(num_epochs)\n",
    "        bce_losses = np.zeros(num_epochs)\n",
    "        kldes = np.zeros(num_epochs)\n",
    "        \n",
    "#        d_losses_val = np.zeros(num_epochs)\n",
    "        precision_abn = np.zeros(num_epochs)\n",
    "        recall_abn = np.zeros(num_epochs)\n",
    "        \n",
    "        \n",
    "        self.temperature = 1.\n",
    "        \n",
    "        total_steps = (len(data_loader.dataset) // batch_size) #*num_epochs\n",
    "        print(\"[INFO] Starting training phase...\")\n",
    "        start = time()\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            step_count = 0\n",
    "            for epoch in range(num_epochs):\n",
    "                self.D.train()\n",
    "                self.G.train()\n",
    "                i = 0\n",
    "                for batch in data_loader:\n",
    "                    \n",
    "                    step_count += 1\n",
    "                    batch = batch.to(self.device)\n",
    "\n",
    "                    ### Train discriminator ###\n",
    "                    for _ in range(num_q_steps):\n",
    "                        d_loss, real_score, fake_score = self.D_step(batch,step_count)\n",
    "\n",
    "                    ### Train Generator ###\n",
    "                    for _ in range(num_g_steps):\n",
    "                        g_loss, bce_loss, rec_loss, kl = self.G_step(batch,step_count)\n",
    "\n",
    "                    d_losses[epoch] = d_losses[epoch]*(i/(i+1.)) + d_loss.item()*(1./(i+1.))\n",
    "                    g_losses[epoch] = g_losses[epoch]*(i/(i+1.)) + g_loss.item()*(1./(i+1.))\n",
    "                    \n",
    "                    rec_losses[epoch] = rec_losses[epoch]*(i/(i+1.)) + rec_loss.item()*(1./(i+1.)) \n",
    "                    bce_losses[epoch] = bce_losses[epoch]*(i/(i+1.)) + bce_loss.item()*(1./(i+1.)) \n",
    "                    kldes[epoch] = kldes[epoch]*(i/(i+1.)) + kl.item()*(1./(i+1.))\n",
    "                    \n",
    "                    real_scores[epoch] = real_scores[epoch]*(i/(i+1.)) + real_score.mean().item()*(1./(i+1.))\n",
    "                    fake_scores[epoch] = fake_scores[epoch]*(i/(i+1.)) + fake_score.mean().item()*(1./(i+1.))\n",
    "\n",
    "                    # Anneal the temperature along with training steps\n",
    "                    self.anneal_temp(lowerbnd)\n",
    "                    \n",
    "                    i += 1\n",
    "                \n",
    "#                dLossVal = self.evaluation(test_loader)\n",
    "                \n",
    "#                d_losses_val[epoch] = dLossVal\n",
    "               \n",
    "                sys.stdout.write(\"\\r\" + 'Epoch [{:>3}/{}] | d_loss: {:.4f} | g_loss: {:.4f} ({:.2f}, {:.2f}, {:.2f}) | D(x): {:.2f} | D(G(x)): {:.2f} '\n",
    "                              .format(epoch+1, num_epochs, d_losses[epoch], g_losses[epoch], bce_loss.item(), rec_losses[epoch], kldes[epoch], real_scores[epoch], fake_scores[epoch]))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            #training is finished, now I log my data\n",
    "            \n",
    "            self.G.eval()\n",
    "#            print(\"prova0\\n\")\n",
    "            counter=0\n",
    "            for batch in data_loader:\n",
    "                if (counter==0):\n",
    "                    counter=counter + 1\n",
    "                    logits, z_mean, z_logvar, a=self.G(batch, self.discreteCol, self.selected_columns,\n",
    "                             self.index, self.temperature)\n",
    "                    a=a.detach().numpy()\n",
    "                else:\n",
    "                    logits, z_mean, z_logvar, b=self.G(batch, self.discreteCol, \n",
    "                                                       self.selected_columns,self.index, self.temperature)\n",
    "                    b=b.detach().numpy()\n",
    "                    a=np.vstack((a, b))\n",
    "            np.save(\"cicids_created_data.npy\", a)\n",
    "            np.save(\"/home/notebook/attack_generation/saved_attacks_generated/cicids/arn.npy\", a)            \n",
    "        except KeyboardInterrupt:\n",
    "            print('-' * 89)\n",
    "            print('[INFO] Exiting from training early')\n",
    "        print(f'\\n[INFO] Training phase... Elapsed time: {(time() - start):.0f} seconds\\n')\n",
    "        return d_losses[:epoch], g_losses[:epoch],rec_losses[:epoch], bce_losses[:epoch], kldes[:epoch], real_scores[:epoch], fake_scores[:epoch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1616573917983,
     "user": {
      "displayName": "Angelica Liguori",
      "photoUrl": "",
      "userId": "05017064839845812601"
     },
     "user_tz": -60
    },
    "id": "nvzpyul-PRID"
   },
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13436,
     "status": "ok",
     "timestamp": 1616573932393,
     "user": {
      "displayName": "Angelica Liguori",
      "photoUrl": "",
      "userId": "05017064839845812601"
     },
     "user_tz": -60
    },
    "id": "lV-_QYpwPRID",
    "outputId": "49afc328-2bf7-4070-ae8f-4cee2e471783",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67494\n",
      "67494 normal records, 132503 anormal records\n",
      "We use 66251 anomalous records\n",
      "-----------------------------------------------------------------------------------------\n",
      "There are 40496 records in training set\n",
      "Training set is composed by 40496 normal records and 0 abnormal records\n",
      "-----------------------------------------------------------------------------------------\n",
      "There are 66251 records in validation set\n",
      "Validation set is composed by 0 normal records and 66251 abnormal records\n",
      "-----------------------------------------------------------------------------------------\n",
      "There are 93250 records in test set\n",
      "Test set is composed by 26998 normal records and 66252 abnormal records\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "PATH=\"/home/notebook/attack_generation/datasets/CICIDS18_Shuffled_Reduced.csv\"\n",
    "dataset = get_CICIDS(PATH, seed, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40496, 101)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['x_train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40496, 101)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['x_train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "train_loader = DataLoader(dataset=torch.FloatTensor(dataset['x_train']).to(device), \n",
    "                      batch_size = batch_size, shuffle=True, drop_last = True)\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.tensor(dataset['x_val']).to(device), \n",
    "                                          torch.tensor(dataset['y_val'].to_numpy()).to(device))\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size = batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(dataset['x_test']).to(device), \n",
    "                                          torch.tensor(dataset['y_test'].to_numpy()).to(device))\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size = batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(dataset['x_test'][:,2], dataset['y_test'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40496, 101)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['x_train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100112    0.0\n",
       "102675    0.0\n",
       "108612    0.0\n",
       "2781      0.0\n",
       "29449     0.0\n",
       "         ... \n",
       "139604    0.0\n",
       "41231     0.0\n",
       "153598    0.0\n",
       "139925    0.0\n",
       "34127     0.0\n",
       "Name: label, Length: 40496, dtype: float32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['y_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154156    0.0\n",
       "168615    0.0\n",
       "101272    0.0\n",
       "10624     0.0\n",
       "13810     0.0\n",
       "         ... \n",
       "166300    1.0\n",
       "180973    1.0\n",
       "156434    1.0\n",
       "199097    1.0\n",
       "184110    1.0\n",
       "Name: label, Length: 93250, dtype: float32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.06273329e-01, 1.18033891e-03, 8.66050832e-04, 4.91028139e-03,\n",
       "       4.06954030e-04, 1.32191777e-01, 0.00000000e+00, 1.47945201e-02,\n",
       "       7.00384825e-02, 7.77422786e-01, 0.00000000e+00, 4.64331657e-01,\n",
       "       8.61697435e-01, 2.50991320e-06, 1.97521814e-07, 1.91377494e-02,\n",
       "       5.36815673e-02, 8.69069844e-02, 1.10253584e-08, 5.06666660e-01,\n",
       "       4.78443727e-02, 9.09658298e-02, 8.69069844e-02, 2.42557888e-07,\n",
       "       5.05833328e-01, 3.59415933e-02, 1.11671537e-01, 9.90291238e-02,\n",
       "       1.18343193e-08, 9.39759018e-04, 1.01449154e-03, 8.23007511e-08,\n",
       "       1.72831577e-07, 0.00000000e+00, 7.77422786e-01, 2.68159926e-01,\n",
       "       8.06185246e-01, 6.49934709e-01, 2.36236140e-01, 1.47945201e-02,\n",
       "       4.64331657e-01, 1.18033891e-03, 4.91028139e-03, 8.66050832e-04,\n",
       "       4.07314044e-04, 1.25015259e-01, 1.90734863e-03, 5.13572991e-03,\n",
       "       5.00000000e-01, 2.92160665e-04, 9.92239220e-04, 1.23917114e-03,\n",
       "       1.02297294e-04, 9.80582535e-02, 3.30474251e-03, 9.90291238e-02,\n",
       "       9.60867852e-02, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "       1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       0.00000000e+00, 1.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "       1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "       0.00000000e+00, 1.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "       1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['x_train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 1.0557684e-09, 1.0590241e-09, ..., 1.7354732e+02,\n",
       "       1.8100146e+02, 2.2716655e+02], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(dataset['x_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 400 #400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Protocol', 'Fwd_PSH_Flags', 'Bwd_PSH_Flags', 'Fwd_URG_Flags', 'Bwd_URG_Flags', 'FIN_Flag_Cnt', 'SYN_Flag_Cnt', 'RST_Flag_Cnt', 'PSH_Flag_Cnt', 'ACK_Flag_Cnt', 'URG_Flag_Cnt', 'CWE_Flag_Count', 'ECE_Flag_Cnt', 'DownUp_Ratio', 'Fwd_Bytsb_Avg', 'Fwd_Pktsb_Avg', 'Fwd_Blk_Rate_Avg', 'Bwd_Bytsb_Avg', 'Bwd_Pktsb_Avg', 'Bwd_Blk_Rate_Avg'])\n",
      "['Protocol', 'Fwd_PSH_Flags', 'Bwd_PSH_Flags', 'Fwd_URG_Flags', 'Bwd_URG_Flags', 'FIN_Flag_Cnt', 'SYN_Flag_Cnt', 'RST_Flag_Cnt', 'PSH_Flag_Cnt', 'ACK_Flag_Cnt', 'URG_Flag_Cnt', 'CWE_Flag_Count', 'ECE_Flag_Cnt', 'DownUp_Ratio', 'Fwd_Bytsb_Avg', 'Fwd_Pktsb_Avg', 'Fwd_Blk_Rate_Avg', 'Bwd_Bytsb_Avg', 'Bwd_Pktsb_Avg', 'Bwd_Blk_Rate_Avg']\n",
      "[INFO] Starting training phase...\n",
      "Epoch [400/400] | d_loss: 0.0138 | g_loss: 0.1115 (6.99, 0.04, 0.93) | D(x): 0.99 | D(G(x)): 0.00 \n",
      "[INFO] Training phase... Elapsed time: 1192 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = AADNet(device, dataset['selectedColumns'], dataset['discreteCol'], dataset['index'],\n",
    "                 dataset['x_train'].shape[1])\n",
    "\n",
    "print(dataset['selectedColumns'].keys())\n",
    "print(dataset['discreteCol'])\n",
    "d_losses,g_losses,rec_losses,bce_losses,kldes,real_scores,fake_scores=trainer.train(data_loader=train_loader,\n",
    "                test_loader=train_loader,\n",
    "                batch_size = 256,\n",
    "                num_epochs = num_epochs,\n",
    "                step = 40,\n",
    "                lowerbnd=5e-20,\n",
    "                num_q_steps = 1, \n",
    "                num_g_steps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.G(1024, dataset['discreteCol'], dataset['selectedColumns'], dataset['index'],dataset['x_train'].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp, tn, fp, fn, accuracy, mcc:\n",
      "45988 26759 239 20264 0.7801286863270778 0.621632218084582\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "\n",
    "prediction, y_true, yP = trainer.predict(test_loader)\n",
    "prediction=np.where(prediction==0, 1, 0)\n",
    "\n",
    "accuracy=accuracy_score(y_true, prediction)\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, prediction).ravel()\n",
    "mcc=matthews_corrcoef(y_true, prediction)\n",
    "print(\"tp, tn, fp, fn, accuracy, mcc:\")\n",
    "print(tp, tn, fp, fn, accuracy, mcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_in_training_set=np.unique(dataset['x_train'].shape[0])\n",
    "original_attack_in_training_set=np.sum(dataset['y_train'])\n",
    "normal_in_test_set=np.unique(dataset['y_test'], return_counts=True)[1][0]\n",
    "attacks_in_test_set=np.unique(dataset['y_test'], return_counts=True)[1][1]\n",
    "generated_attacks=np.load(\"/home/notebook/attack_generation/saved_attacks_generated/cicids/arn.npy\").shape[0]\n",
    "PATH='/data/notebook/ganzata_data/'\n",
    "cicids_competitors=open(PATH+\"cicids_competitors.csv\", \"a\")\n",
    "cicids_competitors.write('cicids, '+\n",
    "                       'ARN, '+\n",
    "                       'ARN, '+\n",
    "                       '0.6--0.4, '+\n",
    "                       str(normal_in_training_set)+', '+\n",
    "                       str(original_attack_in_training_set)+', '+\n",
    "                       str(generated_attacks)+', '+\n",
    "                       str(normal_in_test_set)+', '+\n",
    "                       str(attacks_in_test_set)+', '+\n",
    "                       str(dataset['x_test'].shape[1])+', '+\n",
    "                       ' NO AUGMENTATION, '+\n",
    "                       '{}, {}, {}, {}, {:3f}, {:3f} \\n'.format(tp, tn, fp, fn, accuracy, mcc))\n",
    "cicids_competitors.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copia di modelloDeep.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
